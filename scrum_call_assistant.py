# -*- coding: utf-8 -*-
"""scrum_call_assistant.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pmd38XfI228FmGIScNMINQT2VIxjJHy1

## Install libraries
"""

!pip install --upgrade gspread oauth2client
!pip install pinecone-client
!pip install --upgrade torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
!pip install sentence-transformers
!pip install streamlit
!pip install streamlit pyngrok
!pip install gradio pyngrok sentence-transformers pandas

"""## Import libraries"""

import gspread
from oauth2client.service_account import ServiceAccountCredentials
from google.colab import files
from datetime import datetime
import pandas as pd
import numpy as np
import random
from sentence_transformers import SentenceTransformer, util
import os
import pinecone
from pinecone import Pinecone
import gradio as gr
from sklearn.metrics.pairwise import cosine_similarity

"""## Authenticate and connect to Google Sheets"""

uploaded = files.upload() # Manually upload your service_account.json file here

# Replace 'your_service_account.json' with the name of the uploaded JSON key file
scope = ["https://spreadsheets.google.com/feeds", "https://www.googleapis.com/auth/drive"]
creds = ServiceAccountCredentials.from_json_keyfile_name('scrum-call-assistant-a8750ee31f37.json', scope)
client = gspread.authorize(creds)


# Function to create a new worksheet for each day
def create_daily_sheet():
    # Get today's date in YYYY-MM-DD format
    today_date = datetime.now().strftime("%Y-%m-%d")

    # Check if a worksheet with today's date already exists
    try:
        worksheet = spreadsheet.worksheet(today_date)
        print(f"Worksheet for {today_date} already exists.")
    except gspread.exceptions.WorksheetNotFound:
        # Create a new worksheet for today if it doesn't exist
        worksheet = spreadsheet.add_worksheet(title=today_date, rows="100", cols="5")
        headers = ["Project Name", "Project ID", "Employee Names", "Updates", "Blockers/Queries"]
        worksheet.append_row(headers)
        print(f"New worksheet created for {today_date}.")

    return worksheet

spreadsheet = client.create("Employee Updates Tracker")
spreadsheet.share('sudiksha-chindula@scrum-call-assistant.iam.gserviceaccount.com', perm_type='user', role='writer')  # Share with your email or anyone who needs access
spreadsheet.share('sudiksha.chindula7@gmail.com', perm_type='user', role='writer')

daily_sheet = create_daily_sheet()

worksheet = spreadsheet.get_worksheet(0)
worksheet.update_title("Daily Updates")



print(f"Spreadsheet created: {spreadsheet.url}")

"""## Sample entries for the day"""

import random
from datetime import datetime, timedelta
import gspread
from oauth2client.service_account import ServiceAccountCredentials

# Authenticate with Google Sheets
scope = ["https://spreadsheets.google.com/feeds", "https://www.googleapis.com/auth/drive"]
creds = ServiceAccountCredentials.from_json_keyfile_name('scrum-call-assistant-a8750ee31f37.json', scope)
client = gspread.authorize(creds)

# Open the main spreadsheet and create or access today's worksheet
spreadsheet = client.open("Employee Updates Tracker")
today_date = datetime.now().strftime("%Y-%m-%d")

# Function to get or create the daily sheet
def get_or_create_daily_sheet():
    try:
        worksheet = spreadsheet.worksheet(today_date)
        print(f"Worksheet for {today_date} already exists.")
    except gspread.exceptions.WorksheetNotFound:
        worksheet = spreadsheet.add_worksheet(title=today_date, rows="100", cols="5")
        headers = ["Project Name", "Project ID", "Employee Names", "Updates", "Blockers/Queries"]
        worksheet.append_row(headers)
        print(f"New worksheet created for {today_date}.")
    return worksheet

# Access today's worksheet
worksheet = get_or_create_daily_sheet()

# Sample data generation for 'Employee Updates Tracker'
projects = ["Data Pipeline Enhancement", "Web App Redesign", "API Integration", "Machine Learning Model", "Backend Optimization"]
employees = ["Alice", "Bob", "Charlie", "Diana", "Evan"]
blockers = ["None", "Waiting for data access", "Dependency on API update", "Awaiting feedback", "Issue with deployment"]
updates = [
    "Completed initial setup.",
    "Working on API endpoints.",
    "Refactoring code for efficiency.",
    "Testing the latest model.",
    "Resolving deployment issues.",
    "Code review completed.",
    "Fixed bugs reported in QA.",
    "Integrating third-party API.",
    "Researching optimization techniques.",
    "Finalizing documentation."
]

# Generate and add 25 entries to today's worksheet
for i in range(10):
    project = random.choice(projects)
    employee = random.choice(employees)
    update = random.choice(updates)
    blocker = random.choice(blockers)
    project_id = f"PID-{random.randint(1000, 9999)}"

    # Prepare the row data
    row = [project, project_id, employee, update, blocker]

    # Append the row to the worksheet
    worksheet.append_row(row)
    print(f"Added entry: {row}")

print(f"Entries successfully added to the worksheet for {today_date}.")

"""## Script to fetch data"""

'''
COMPLETE CODE FOR A FILE ALREADY CREATED

import gspread
from oauth2client.service_account import ServiceAccountCredentials
from datetime import datetime
import pandas as pd

# Authenticate with Google Sheets
scope = ["https://spreadsheets.google.com/feeds", "https://www.googleapis.com/auth/drive"]
creds = ServiceAccountCredentials.from_json_keyfile_name('scrum-call-assistant-a8750ee31f37.json', scope)
client = gspread.authorize(creds)

# Open the main spreadsheet
spreadsheet = client.open("Employee Updates Tracker")
'''

# Function to extract data from today's worksheet
def extract_today_data():
    today_date = datetime.now().strftime("%Y-%m-%d")
    try:
        worksheet = spreadsheet.worksheet(today_date)
        # Get all records as dictionaries
        records = worksheet.get_all_records()
        print(f"Data for {today_date} extracted successfully.")
        # Convert to a pandas DataFrame for tabular format (optional)
        data_df = pd.DataFrame(records)
        return records, data_df
    except gspread.exceptions.WorksheetNotFound:
        print(f"No worksheet found for {today_date}.")
        return [], None



# Extract data for today in list and DataFrame format
today_data_list, today_data_df = extract_today_data()

# Print today's data in list format
print("Today's Data List:")
print(today_data_list)

# Print today's data in tabular format (if using pandas DataFrame)
if today_data_df is not None:
    print("\nToday's Data Table:")
    print(today_data_df)

"""## Loading Embedding Moder and Connecting to a Vector Database"""

# Load SBERT model for embeddings
model = SentenceTransformer('all-MiniLM-L6-v2')

os.environ["PINECONE_API_KEY"] = "pcsk_51kFav_LVbf4tK5bnDvjEkWoFQnLq5iBagRstRhoVR2d3UjAkVty2uCxNCo7Csv9aXzZjP"
os.environ["PINECONE_ENVIRONMENT"] = "aped-4627-b74a"
os.environ["PINECONE_INDEX_NAME"] = "employee-updates-tracker"  # Ensure this line is run to set the index name

# Initialize Pinecone without using init()

index_name = os.getenv("PINECONE_INDEX_NAME", "employee-updates-tracker")  # Default to "text-embeddings" if not set
pc = Pinecone(api_key=os.environ.get("PINECONE_API_KEY"), environment=os.environ.get("PINECONE_ENVIRONMENT"))

index = pc.Index(index_name)

# Access the index
index = pc.Index(index_name)

# Chunk data into manageable pieces if necessary
def chunk_text(text, max_chunk_size=100):
    return [text[i:i + max_chunk_size] for i in range(0, len(text), max_chunk_size)]

# Process and embed daily DataFrame
def embed_daily_data(data_df):
    index_vectors = []
    for _, row in data_df.iterrows():
        combined_text = f"Project: {row['Project Name']} | Update: {row['Updates']} | Blockers: {row['Blockers/Queries']}"
        chunks = chunk_text(combined_text, max_chunk_size=100)
        for chunk in chunks:
            vector = model.encode(chunk).tolist()
            index_vectors.append((f"{row['Project ID']}_{chunk}", vector))

    # Insert vectors into Pinecone index
    index.upsert(vectors=index_vectors)
    print(f"{len(index_vectors)} vectors upserted to Pinecone index.")

# Define the query_employee_updates function
def query_employee_updates(query, top_k=10):
    # Embed the query
    query_chunks = chunk_text(query, max_chunk_size=100)
    query_embeddings = [model.encode(chunk) for chunk in query_chunks]

    results = []
    for embedding in query_embeddings:
        pinecone_results = index.query(vector=embedding.tolist(), top_k=top_k, include_metadata=True)
        results.extend(pinecone_results['matches'])

    # Retrieve matching rows from the DataFrame based on Project ID
    relevant_rows = []
    for match in results:
        print("Match found:", match)  # Debug to see each match
        project_id = match['id'].split("_")[0]
        row = today_data_df[today_data_df['Project ID'] == project_id]
        if not row.empty:
            relevant_rows.append(row)

    return pd.concat(relevant_rows).drop_duplicates() if relevant_rows else pd.DataFrame()

# Check today's data in DataFrame
#print("Today's Data DataFrame:")
#print(today_data_df)

# Embed and index data
embed_daily_data(today_data_df)

# Example Query
query_text = "Issue with deployment"
print("\nRunning query:", query_text)

# Execute query
relevant_data = query_employee_updates(query_text)

# Display results
if not relevant_data.empty:
    print("\nRelevant Data:")
    print(relevant_data)
else:
    print("No relevant data found.")

"""## Integrating LLM and Streamlit UI"""

from sentence_transformers import SentenceTransformer
import numpy as np

# Load the SentenceTransformer model
model = SentenceTransformer('all-MiniLM-L6-v2')

# Check if embeddings already exist in the DataFrame; otherwise, compute them
if 'embedding' not in today_data_df.columns:
    today_data_df['embedding'] = today_data_df['Updates'].apply(lambda x: model.encode(x))
else:
    print("Embeddings already exist in the DataFrame.")

from sklearn.metrics.pairwise import cosine_similarity

# Function to embed a query and find relevant data
def get_relevant_data(query_text):
    # Embed the query using the loaded model
    query_embedding = model.encode(query_text).reshape(1, -1)

    # Calculate cosine similarity between query and each row embedding
    df_embeddings = np.vstack(today_data_df['embedding'].values)
    similarities = cosine_similarity(query_embedding, df_embeddings).flatten()

    # Get top result(s) based on similarity
    top_indices = similarities.argsort()[-3:][::-1]  # Retrieve top 3 results
    relevant_rows = today_data_df.iloc[top_indices]
    return relevant_rows

import pandas as pd
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
import gradio as gr
'''
# Define sample data
data = {
    "Project Name": [
        "API Integration", "Machine Learning Model", "Web App Redesign",
        "Backend Optimization", "Data Pipeline Enhancement"
    ],
    "Project ID": [
        "PID-4741", "PID-7014", "PID-6011", "PID-3843", "PID-3015"
    ],
    "Employee Names": [
        "Alice", "Bob", "Evan", "Diana", "Charlie"
    ],
    "Updates": [
        "Resolving deployment issues.", "Researching optimization techniques.",
        "Fixed bugs reported in QA.", "Completed initial setup.",
        "Integrating third-party API."
    ],
    "Blockers/Queries": [
        "Dependency on API update", "None", "Awaiting feedback",
        "Dependency on API update", "None"
    ]
}

# Create the DataFrame
today_data_df = pd.DataFrame(data)
'''
# Load the SentenceTransformer model
model = SentenceTransformer('all-MiniLM-L6-v2')

# Compute embeddings for each row in the DataFrame (only if not already present)
if 'embedding' not in today_data_df.columns:
    today_data_df['embedding'] = today_data_df['Updates'].apply(lambda x: model.encode(x))
else:
    print("Embeddings already exist in the DataFrame.")

# Function to embed a query and find relevant data
def get_relevant_data(query_text):
    # Embed the query using the loaded model
    query_embedding = model.encode(query_text).reshape(1, -1)

    # Calculate cosine similarity between query and each row embedding
    today_data_embeddings = np.vstack(today_data_df['embedding'].values)
    similarities = cosine_similarity(query_embedding, today_data_embeddings).flatten()

    # Get top result(s) based on similarity
    top_indices = similarities.argsort()[-3:][::-1]  # Retrieve top 3 results
    relevant_rows = today_data_df.iloc[top_indices]
    return relevant_rows

# Chatbot response function

def chatbot_response(history, user_input):
    # Get relevant data from the DataFrame based on the embedded query
    relevant_data = get_relevant_data(user_input)

    if relevant_data.empty:
        bot_reply = "No relevant data found."
    else:
        # Format the response based on relevant data
        bot_reply = ""
        for _, row in relevant_data.iterrows():
            bot_reply += f"**Project Name**: {row['Project Name']}\n"
            bot_reply += f"**Project ID**: {row['Project ID']}\n"
            bot_reply += f"**Employee Name**: {row['Employee Names']}\n"
            bot_reply += f"**Update**: {row['Updates']}\n"
            bot_reply += f"**Blockers/Queries**: {row['Blockers/Queries']}\n\n"
        bot_reply = bot_reply.strip()

    # Append the user's input and bot's reply to the chat history
    history.append(["user", user_input])
    history.append(["bot", bot_reply])

    return history, history

# Define the Gradio interface for the chatbot
with gr.Blocks() as demo:
    gr.Markdown("<h1 style='text-align: center;'>Employee Updates Chatbot</h1>")
    chatbot = gr.Chatbot()
    msg = gr.Textbox(placeholder="Type a message...", label="Enter your query:")
    submit = gr.Button("Send")

    # Set the function with inputs and outputs for the chatbot
    submit.click(fn=chatbot_response, inputs=[chatbot, msg], outputs=[chatbot, chatbot])

# Launch the Gradio app with sharing enabled
demo.launch(share=True)